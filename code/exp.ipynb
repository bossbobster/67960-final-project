{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731d7ff5",
   "metadata": {},
   "source": [
    "# **MoE Experiments**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03f6e9",
   "metadata": {},
   "source": [
    "First we just want to import the `Transformer` class as well as all of our custom MoE modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f815a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborne as sns\n",
    "import time\n",
    "\n",
    "from transformer import Transformer\n",
    "from moes import RegularMoE, RandomMoE, OrthogonalMoE, HashMoE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cpu\" and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.mps.manual_seed(67960)\n",
    "if device.type == \"cuda\" or device.type == \"cpu\":\n",
    "    torch.manual_seed(67960)\n",
    "\n",
    "B = 32\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156862f",
   "metadata": {},
   "source": [
    "Next, we want to import our data and get it ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd0b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AG News dataset...\n",
      "Loading GPT-2 tokenizer...\n",
      "Vocab size: 50257\n",
      "Train samples: 120000, Test samples: 7600\n",
      "\n",
      "*Data done loading*\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# load dataset\n",
    "print(\"Loading AG News dataset...\")\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "# use GPT-2's BPE tokenizer\n",
    "print(\"Loading GPT-2 tokenizer...\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # set EOS as padding\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Train samples: {len(train_data)}, Test samples: {len(test_data)}\")\n",
    "\n",
    "# dataset class with BPE tokenization\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, max_len=128):\n",
    "        self.data = hf_dataset\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['text']\n",
    "        encoding = tokenizer(text, truncation=True, max_length=self.max_len+1, padding='max_length', return_tensors='pt')\n",
    "        tokens = encoding['input_ids'].squeeze(0)\n",
    "        \n",
    "        x = tokens[:-1]\n",
    "        y = tokens[1:]\n",
    "        mask = (x != tokenizer.pad_token_id)\n",
    "        return x, y, mask\n",
    "\n",
    "# create dataloaders\n",
    "train_dataset = TextDataset(train_data, max_len=128)\n",
    "test_dataset = TextDataset(test_data, max_len=128)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=B, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=B, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"\\n*Data done loading*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e622b17",
   "metadata": {},
   "source": [
    "Before we do anything, we want to set up some constants and create our `Transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e452906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D: 128\n",
      " H: 256\n",
      " N: 8\n",
      " K: 2\n",
      " V: 50257\n",
      " n_heads: 4\n",
      " n_layers: 2\n",
      " max_seq_len: 128\n",
      "Model 1 (RegularMoE) has 8152464 parameters and 8152464 trainable parameters\n",
      "Model 2 (RandomMoE) has 8152464 parameters and 8150400 trainable parameters\n",
      "Model 3 (OrthogonalMoE) has 8150400 parameters and 8150400 trainable parameters\n",
      "Model 4 (HashMoE) has 8150400 parameters and 8150400 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "D = 128\n",
    "H = 256\n",
    "N = 8\n",
    "K = 2\n",
    "V = vocab_size\n",
    "n_heads = 4\n",
    "n_layers = 2\n",
    "max_seq_len = 128\n",
    "\n",
    "print(f\"D: {D}\\n H: {H}\\n N: {N}\\n K: {K}\\n V: {V}\\n n_heads: {n_heads}\\n n_layers: {n_layers}\\n max_seq_len: {max_seq_len}\")\n",
    "\n",
    "# create models\n",
    "moe_fns = [\n",
    "    lambda: RegularMoE(D, H, N, K),\n",
    "    lambda: RandomMoE(D, H, N, K),\n",
    "    lambda: OrthogonalMoE(D, H, N, K),\n",
    "    lambda: HashMoE(D, H, N, K)\n",
    "]\n",
    "models = [Transformer(V, D, n_heads, n_layers, moe_fn, max_seq_len).to(device) for moe_fn in moe_fns]\n",
    "\n",
    "# print number of parameters in each model\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"Model {i+1} ({moe_fns[i]().__class__.__name__}) has {sum(p.numel() for p in model.parameters())} parameters and {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad42aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d96194c9",
   "metadata": {},
   "source": [
    "Now we can finally train all of our models separately and then compare their results to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ec1b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Model 1/4: RegularMoE\n",
      "============================================================\n",
      "3750 batches to process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=96, pipe_handle=177)\u001b[0m\n",
      "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "\u001b[1;35mAttributeError\u001b[0m: \u001b[35mCan't get attribute 'TextDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 53026) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/67960-final-project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1274\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/queues.py:111\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    110\u001b[39m timeout = deadline - time.monotonic()\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/connection.py:1148\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n\u001b[32m   1147\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1148\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/selectors.py:398\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/67960-final-project/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[39m, in \u001b[36m_set_SIGCHLD_handler.<locals>.handler\u001b[39m\u001b[34m(signum, frame)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandler\u001b[39m(signum, frame):\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid 53026) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m start_time = time.time()\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m train_losses.append(train_loss)\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[142]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, device)\u001b[39m\n\u001b[32m      5\u001b[39m num_batches = \u001b[32m0\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m batches to process...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# tm1 = time.time()\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/67960-final-project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/67960-final-project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/67960-final-project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/67960-final-project/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1288\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1287\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1289\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1290\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1292\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 53026) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch, return average loss\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    print(f\"{len(loader)} batches to process...\")\n",
    "    \n",
    "    for x, y, mask in loader:\n",
    "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "        \n",
    "        # tm1 = time.time()\n",
    "        logits = model(x, mask)  # [B, S, V]\n",
    "        # tm2 = time.time()\n",
    "        # print(f\"Time taken for forward pass: {tm2 - tm1:.4f}s\")\n",
    "        \n",
    "        loss = F.cross_entropy(logits.view(-1, V), y.view(-1), ignore_index=tokenizer.pad_token_id)\n",
    "        \n",
    "        # tm1 = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        # tm2 = time.time()\n",
    "        # print(f\"Time taken for backward pass: {tm2 - tm1:.4f}s\")\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        if num_batches % 100 == 0:\n",
    "            print(f\"Processed {num_batches} batches...\")\n",
    "    return total_loss / num_batches\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate on dataset, return average loss\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for x, y, mask in loader:\n",
    "        x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "        logits = model(x, mask)\n",
    "        loss = F.cross_entropy(logits.view(-1, V), y.view(-1), ignore_index=tokenizer.pad_token_id)\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "# Training config\n",
    "num_epochs = 3\n",
    "lr = 3e-4\n",
    "model_names = [models[i].moe_fn().__class__.__name__ for i in range(len(models))]\n",
    "\n",
    "# Train each model\n",
    "results = {}\n",
    "for i, (model, name) in enumerate(zip(models, model_names)):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training Model {i+1}/{len(models)}: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss = evaluate(model, test_loader, device)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Test Loss: {test_loss:.4f} | Time: {epoch_time:.2f}s\")\n",
    "    \n",
    "    results[name] = {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_test_loss': test_losses[-1]\n",
    "    }\n",
    "    \n",
    "    # Move back to CPU to free memory\n",
    "    model = model.cpu()\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "for name, res in results.items():\n",
    "    print(f\"{name:20s} | Train: {res['final_train_loss']:.4f} | Test: {res['final_test_loss']:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for name, res in results.items():\n",
    "    ax1.plot(range(1, num_epochs+1), res['train_losses'], marker='o', label=name)\n",
    "    ax2.plot(range(1, num_epochs+1), res['test_losses'], marker='o', label=name)\n",
    "\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Test Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
